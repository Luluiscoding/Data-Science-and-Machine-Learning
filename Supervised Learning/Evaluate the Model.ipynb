{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****\n",
    "\n",
    "**Outline**\n",
    "\n",
    "- Evaluate a model\n",
    "- Model Selection and training/cross validation/test sets\n",
    "- Bias and Variance\n",
    "  - Diagnosing bias and variance\n",
    "  - Regularization and bias/variance\n",
    "- Error Analysis\n",
    "  - What error analysis do?\n",
    "  - Misclassified spam email example\n",
    "\n",
    "****"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the model\n",
    "\n",
    "our example is predict housing prices.  \n",
    "    \n",
    "Here is our model:   \n",
    "  \n",
    "$\\begin{array}{l}f_{\\vec{w}} \\cdot b(\\vec{x})=w_1 x+w_2 x^2+\\cdots+w_n x^n+b \\\\ x_1=\\text { size in feet } \\\\ x_2=\\text { number of bedrooms } \\\\ x_3=\\text { number of floors } \\\\ x_4=\\text { age of home in years. }\\end{array}$  \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"image/curve.png\" width=\"200\">  \n",
    "\n",
    "This curve fits the training data really well. But, we don't like this model very much because even though the model fits the training data well, we think it will fail to generalize to new examples that aren't in the training set.  \n",
    "\n",
    "Here is a simple dataset we use:  \n",
    "<img src=\"image/dataset.png\" width=\"200\">  \n",
    "\n",
    "we can divide the data into training set(70%) and test set(30%).  \n",
    "$m_{train}= \\text{number of training examples}$   \n",
    "So, the training set is $[(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),\\cdots,(x^{(m_{train})},y^{(m_{train})})]$   \n",
    "$m_{test}=\\text{number of test examples}$    \n",
    "So, the test set is $[(x_{test}^{(1)},y_{test}^{(1)}),(x_{test}^{(2)},y_{test}^{(2)}),\\cdots,(x_{test}^{(m_{test})},y_{test}^{(m_{test})})]$  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following calculation, $M$ stands for $m_{train}$ and $m$ stands for $m_{test}$.  \n",
    "  \n",
    "Fit parametrs by minimizing cost function:  \n",
    "$J(\\overrightarrow{\\mathrm{w}}, b)=\\left[\\frac{1}{2 m_{\\text {train }}} \\sum_{i=1}^{m_{\\text {train }}}(f_{\\vec{w}, b}\\left(\\vec{x}^{(i)}\\right)-y^{(i)})^2+\\frac{\\lambda}{2 m_{\\text {train }}} \\sum_{j=1}^n w_j^2\\right]$   \n",
    "  \n",
    "compute test error:  \n",
    "$J_{\\text {test }}(\\overrightarrow{\\mathrm{w}}, b)=\\frac{1}{2 m_{\\text {test }}}\\left[\\sum_{i=1}^{m_{\\text {test }}}\\left(f_{\\overrightarrow{\\mathrm{w}}, b}\\left(\\overrightarrow{\\mathrm{x}}_{\\text {test }}^{(i)}\\right)-y_{\\text {test }}^{(i)}\\right)^2\\right]$  \n",
    "  \n",
    "compute training error:   \n",
    "$J_{\\text {train }}(\\overrightarrow{\\mathrm{w}}, b)=\\frac{1}{2 m_{\\text {train }}}\\left[\\sum_{i=1}^{m_{\\text {train }}}\\left(f_{\\overrightarrow{\\mathrm{w}}, b}\\left(\\overrightarrow{\\mathrm{x}}_{\\text {train }}^{(i)}\\right)-y_{\\text {train }}^{(i)}\\right)^2\\right]$   \n",
    "  \n",
    "$J_{train}(w,b)$ will be low, but it is high for the test set.  \n",
    "\n",
    "**Train/test procedure for classification problem**  \n",
    "Fit parametrs by minimizing cost function:  \n",
    "$J(\\vec{w}, b)=-\\frac{1}{m_{\\text {train }}} \\sum_{i=1}^{m_{\\text {train }}}\\left[y^{(i)} \\log \\left(f_{\\vec{w}, b}\\left(\\overrightarrow{\\mathrm{x}}^{(i)}\\right)\\right)+\\left(1-y^{(i)}\\right) \\log \\left(1-f_{\\vec{w}_{, b}}\\left(\\vec{x}^{(i)}\\right)\\right)\\right]+\\frac{\\lambda}{2 m_{\\text {train }}} \\sum_{j=1}^n w_j^2$  \n",
    "compute test error:  \n",
    "$J_{\\text {test }}(\\vec{w}, b)=-\\frac{1}{m_{\\text {test }}} \\sum_{i=1}^{m_{\\text {test }}}[{\\text {test }}^{(i)} \\log \\left(f_{\\vec{w}, b}\\left(\\vec{x}_{\\text {test }}^{(i)}\\right)\\right)+\\left(1-y_{\\text {test }}^{(i)}\\right) \\log \\left(1-f_{\\vec{w}, b}\\left(\\vec{x}_{\\text {test }}^{(i)}\\right)\\right)]$  \n",
    "compute training error:   \n",
    "$J_{\\text {train }}(\\vec{w}, b)=-\\frac{1}{m_{\\text {train }}} \\sum_{i=1}^{m_{\\text {train }}}\\left[y_{\\text {train }}^{(i)} \\log \\left(f_{\\vec{w}, b}\\left(\\vec{x}_{\\text {train }}^{(i)}\\right)\\right)+\\left(1-y_{\\text {train }}^{(i)}\\right) \\log \\left(1-f_{\\vec{w}, b}\\left(\\vec{x}_{\\text {train }}^{(i)}\\right)\\right)\\right]$ "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, for classification problem, you can have the algorithm make a prediction 1 or 0 on every test example. And you can then count up in the test set the fraction of examples where the prediction is not equal to the actual ground truth label. $J_{test}$ would be the fraction of that test set that has been misclassified, $J_{train}$ is a fraction of the training set that has been misclassified. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection and training/cross validation/test sets\n",
    "\n",
    "once the model's parameters $w$ and $b$ have been fit to the training set. The training error may not be a good indicator of how well the algorithm will do or how well it will generalize to new examples that were not in the training set.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are models we can use for this house price prediction question:  \n",
    "$\\text{1.} f_{\\overrightarrow{\\mathrm{w}, b}}(\\overrightarrow{\\mathrm{x}})=w_1 x+b$  \n",
    "$\\text{2.} f_{\\overrightarrow{\\mathrm{w}}, b}(\\overrightarrow{\\mathrm{x}})=w_1 x+w_2 x^2+b$  \n",
    "$\\text{3.} f_{\\overrightarrow{\\mathrm{w}}, b}(\\overrightarrow{\\mathrm{x}})=w_1 x+w_2 x^2+w_3 x^3+b$  \n",
    "$\\vdots$  \n",
    "$\\text{10.} f_{\\overrightarrow{\\mathrm{w}}, b}(\\overrightarrow{\\mathrm{x}})=w_1 x+w_2 x^2+\\cdots+w_{10} x^{10}+b$  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example dataset:  \n",
    "<img src=\"image/dataset.png\" width=\"200\">    \n",
    "We split it into training set(60%), cross validation set(20%) and test set(20%).  \n",
    "For cross validation set, it is $[(x_{cv}^{(1)},y_{cv}^{(1)}),(x_{cv}^{(2)},y_{cv}^{(2)}),\\cdots,(x_{cv}^{(m_{cv})},y_{cv}^{(m_{cv})})]$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Error:  \n",
    "$J_{\\text {train }}(\\overrightarrow{\\mathrm{w}}, b)=\\frac{1}{2 m_{\\text {train }}}\\left[\\sum_{i=1}^{m_{\\text {train }}}\\left(f_{\\overrightarrow{\\mathrm{w}}, b}\\left(\\overrightarrow{\\mathrm{x}}^{(i)}\\right)-y^{(i)}\\right)^2\\right]$  \n",
    "Cross Validation Error:  \n",
    " $J_{c v}(\\overrightarrow{\\mathrm{w}}, b)=\\frac{1}{2 m_{c v}}\\left[\\sum_{i=1}^{m_{c v}}\\left(f_{\\overrightarrow{\\mathrm{w}}, b}\\left(\\overrightarrow{\\mathrm{x}}_{c v}^{(i)}\\right)-y_{c v}^{(i)}\\right)^2\\right]$  \n",
    "Test Error:  \n",
    "$J_{\\text {test }}(\\vec{w}, b)=\\frac{1}{2 m_{\\text {test }}}\\left[\\sum_{i=1}^{m_{\\text {test }}}\\left(f_{\\vec{w}, b}\\left(\\vec{x}_{\\text {test }}^{(i)}\\right)-y_{\\text {test }}^{(i)}\\right)^2\\right]$  \n",
    "  \n",
    "Then we evaluate the model based on cross validation set, which means we choose a model which has the lowest cross validation error.  \n",
    "  \n",
    "In a neural network architecture, we have:  \n",
    "<img src=\"image/NNmodelselection.png\" width=\"300\">  \n",
    "we calculate the cross validation error to choose the best one.  And report it using test set error to see how well it will do.  \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " It's considered best practice in machine learning that if you have to make decisions about your model, such as fitting parameters or choosing the model architecture, such as neural network architecture or degree of polynomial if you're fitting a linear regression, to make all those decisions only using your training set and your cross-validation set, and to not look at the test set at all while you're still making decisions regarding your learning algorithm. It's only after you've come up with one model as your final model to only then evaluate it on the test set and because you haven't made any decisions using the test set, that ensures that your test set is a fair and not overly optimistic estimate of how well your model will generalize to new data.  \n",
    "   \n",
    "This is actually a very widely used procedure. I use this all the time to automatically choose what model to use for a given machine learning application. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias and Variance \n",
    "\n",
    "## Diagnosing bias and variance\n",
    "\n",
    "There are 3 models fitting the data with different polynomial d, which means the number features we use in the model.\n",
    "\n",
    "<img src=\"image/bv1.png\" width=\"700\">  \n",
    "\n",
    "And there is a plot of different performance of degree of d:  \n",
    "\n",
    "<img src=\"image/degree of d.png\" width=\"400\">  \n",
    "\n",
    "- High bias (underfit) : $J_{train}$ will be high and $J_{train} \\approx J_{cv}$  \n",
    "- High Variance (Overfit): $J_{train}$ may be low and $ J_{cv} \\gg J_{train}$\n",
    "- High bias and high variance: $J_{train}$ will be low and $ J_{cv} \\gg J_{train}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization and bias/variance\n",
    "\n",
    "Model: $f_{\\overrightarrow{\\mathrm{w}}, b}(x)=w_1 x+w_2 x^2+w_3 x^3+w_4 x^4+b$  \n",
    "Cost Function: $J(\\overrightarrow{\\mathrm{w}}, b)=\\frac{1}{2 m} \\sum_{i=1}^m\\left(f_{\\overrightarrow{\\mathrm{w}}, b}\\left(\\overrightarrow{\\mathrm{x}}^{(i)}\\right)-y^{(i)}\\right)^2+\\frac{\\lambda}{2 m} \\sum_{j=1}^n w_j^2$  \n",
    "\n",
    "$\\lambda$ is the regularizaiton parameter, its value has impact on bias.\n",
    "\n",
    "When $\\lambda=10000$ (large $\\lambda$), then the model looks like:   \n",
    "<img src=\"image/largelambda.png\" width=\"200\">   \n",
    "When $\\lambda=0$ (small $\\lambda$), then the model looks like:  \n",
    "<img src=\"image/smalllambda.png\" width=\"200\">   \n",
    "If we choose a intermediate $\\lambda$, then the model looks like:  \n",
    "<img src=\"image/intermlambda.png\" width=\"200\">   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the plot about $\\lambda$:  \n",
    "<img src=\"image/lambdabiasvariance.png\" width=\"500\">   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error Analysis \n",
    "\n",
    "## What error analysis do?\n",
    "\n",
    "Let's say you have $m_{cv}=500$ cross validation examples and your algorithm misclassifies 100 of these 500 cross validation examples. The error analysis process just refers to manually looking through these 100 examples and trying to gain insights into where the algorithm is going wrong. And we are going to find a set of examples that the algorithm has misclassified examples from the cross validation set and try to group them into common themes or common properties or common traits.  \n",
    "\n",
    "## Misclassified spam emails example \n",
    "|Types|number|\n",
    "|------------|------------|\n",
    "|Pharma|21|\n",
    "|Deliberate misspelling|3|\n",
    "|Unusual email routing|7|\n",
    "|Steal Password|18|\n",
    "|Spam Message in embedded image|5|"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the table, we can tell that solving deliberate misspelling may not be a good idea. Because it just contribute 3 cases in 100 examples, which means it has small net impact.  \n",
    "  \n",
    "Anothe thing we should notice is that these types are not mutually exclusive. For example, there can be a pharmaceutical spam email that also has unusual routing or a password that has deliberate misspellings and is also trying to carry out the phishing attack.  \n",
    "\n",
    "Now, our example is about 100 misclassified examples out of $m_{cv}=500$. If there is a much larger dataset, say 1000 misclassifications out of 5000 examples, then we can sample from the dataset to derive 100 or couples of 100 examples.  \n",
    "\n",
    "After doing the analysis, we find that a lot of errors are pharmaceutical spam emails, then this might give us some ideas or inspiration for things to do next. For example, we may decide to collect more data of pharmaceutical spam emails so that the learning algorithm can do a better job recognizing these pharmaceutical spam. Or we may come up with some new features that are related to specific names of drugs or specific names of pharmaceutical products of the standards we're trying to sell in order to help us learning algorithm become better at recognizing this type of former spam. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**iterative loop of ML development**  \n",
    "\n",
    "<img src=\"image/iterative loop.png\" width=\"500\"> \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
