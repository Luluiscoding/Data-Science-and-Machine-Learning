{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****\n",
    "\n",
    "**Outline**\n",
    "- Hard voting\n",
    "  - Explanation\n",
    "  - Coding example\n",
    "- Bagging\n",
    "  - Explanation\n",
    "  - Coding example\n",
    "- Random Forest\n",
    "  - Basic RF Algorithm\n",
    "  - Randomizing the feature choice\n",
    "  - Stroke prediction using RF\n",
    "\n",
    "****"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hard voting\n",
    "## Explanation\n",
    "Hard voting is a method for combining the predictions of multiple models to make a final prediction. In hard voting, each model's prediction is treated equally, and the final prediction is made based on the majority vote of all the models.  \n",
    "\n",
    "## Coding example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\C-PY\\Anaconda\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "\n",
    "# Define three different classifiers\n",
    "clf1 = LogisticRegression()\n",
    "clf2 = GaussianNB()\n",
    "clf3 = DecisionTreeClassifier()\n",
    "\n",
    "# Define a voting classifier that uses hard voting\n",
    "voting_clf = VotingClassifier(estimators=[('lr', clf1), ('nb', clf2), ('dt', clf3)], voting='hard')\n",
    "\n",
    "# Fit the voting classifier to the data\n",
    "voting_clf.fit(iris.data, iris.target)\n",
    "\n",
    "# Make predictions using the voting classifier\n",
    "preds = voting_clf.predict(iris.data)\n",
    "\n",
    "# Print the accuracy of the voting classifier\n",
    "print(\"Accuracy:\", voting_clf.score(iris.data, iris.target))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging\n",
    "\n",
    "## Explanation\n",
    "The term bagging referes to bootstrap aggregating. It is a machine learning technique that involves building multiple models from bootstrap samples of the training data and combining their predictions through a voting or averaging mechanism. The idea behind bagging is to reduce the variance of the individual models and improve the overall performance of the ensemble model.\n",
    "\n",
    "In bagging, we randomly select a subset of the training data (with replacement) to train each model in the ensemble. This process creates multiple independent samples of the training data, which are used to train multiple models in parallel. Each model in the ensemble is trained on a different sample of the training data and produces its own set of predictions.\n",
    "\n",
    "Once all the models are trained, their predictions are combined to make a final prediction. This can be done using a simple majority vote (for classification problems) or an average (for regression problems). By combining the predictions of multiple models, we can reduce the risk of overfitting and improve the generalization performance of the ensemble.\n",
    "\n",
    "Bagging is commonly used with decision trees, where it is known as Random Forest. In a Random Forest, each tree in the ensemble is built from a random subset of the features, in addition to a random subset of the training data. This further helps to reduce the variance of the individual trees and improve the accuracy of the ensemble.  \n",
    "\n",
    "## Coding Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\C-PY\\Anaconda\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "\n",
    "# Define a decision tree classifier\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "# Define a bagging classifier with 10 decision tree classifiers\n",
    "bagging_clf = BaggingClassifier(base_estimator=clf, n_estimators=10)\n",
    "\n",
    "# Fit the bagging classifier to the data\n",
    "bagging_clf.fit(iris.data, iris.target)\n",
    "\n",
    "# Make predictions using the bagging classifier\n",
    "preds = bagging_clf.predict(iris.data)\n",
    "\n",
    "# Print the accuracy of the bagging classifier\n",
    "print(\"Accuracy:\", bagging_clf.score(iris.data, iris.target))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "\n",
    "## Basic RF Algorithm\n",
    "\n",
    "Given a training set of size $mï¿¥  \n",
    "\n",
    "```\n",
    "for b = 1 to B: \n",
    "     use sampling with replacement to create a new training set of size m\n",
    "```\n",
    "\n",
    "For example, if There is a dataset contains 10 data, we sampling it with replacement to get a traing set contains 10 examples. Some of them  may be sampled repeatly, but that is ok. Then we will train the decision tree(eg. split by ear shape) on this dataset. Next step, we resampled it and apply the other decision tree(eg. split by face shape). We do this for $B$ times. $B$ is usually between 64 and 228. Then, we get these trees all votes and make a correct final prediction.  \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomizing the feature choice\n",
    "\n",
    "There is one modificaiton to further randomize the feature choice at each node:  \n",
    "at every note when choosing a feature to use to split, if $n$ features are available, rather than picking from all end features, we will instead pick a random subset of $K$ less than $n$ features. And allow the algorithm to choose only from that subset of K features.  \n",
    "A typical choice for the value of $K$ would be to choose it to be $\\sqrt{n}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stroke prediction using RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.960285132382892\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the stroke prediction dataset\n",
    "df = pd.read_csv(\"data/healthcare-dataset-stroke-data.csv\")\n",
    "# Create a LabelEncoder object\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Fit and transform the gender column\n",
    "df['gender'] = le.fit_transform(df['gender'])\n",
    "df['ever_married'] = le.fit_transform(df['ever_married'])\n",
    "df['work_type'] = le.fit_transform(df['work_type'])\n",
    "df['Residence_type'] = le.fit_transform(df['Residence_type'])\n",
    "df['smoking_status'] = le.fit_transform(df['smoking_status'])\n",
    "\n",
    "# Drop rows with missing values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X = df.drop(['stroke'], axis=1)\n",
    "y = df['stroke']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=27)\n",
    "\n",
    "# Create a random forest model with 100 trees\n",
    "model = RandomForestClassifier(n_estimators=70)\n",
    "\n",
    "# Train the model on the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model's accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.960285132382892\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the stroke prediction dataset\n",
    "df = pd.read_csv(\"data/healthcare-dataset-stroke-data.csv\")\n",
    "# Create a LabelEncoder object\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Fit and transform the gender column\n",
    "df['gender'] = le.fit_transform(df['gender'])\n",
    "df['ever_married'] = le.fit_transform(df['ever_married'])\n",
    "df['work_type'] = le.fit_transform(df['work_type'])\n",
    "df['Residence_type'] = le.fit_transform(df['Residence_type'])\n",
    "df['smoking_status'] = le.fit_transform(df['smoking_status'])\n",
    "\n",
    "# Drop null values\n",
    "df = df.dropna()\n",
    "\n",
    "# Select only the 'age' and 'stroke' columns\n",
    "df_num = df[['age', 'stroke']]\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_num[['age']], df_num['stroke'], test_size=0.2, random_state=27)\n",
    "\n",
    "# Create a Random Forest model with 100 trees\n",
    "model = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "# Train the model on the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model's accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
